###### PROJECT OVERVIEW ##########

This project demonstrates an end-to-end Azure Data Engineering pipeline using the AdventureWorks dataset.
It follows a Batch Processing approach to design a modern data architecture that automates data ingestion, transformation, storage, and reporting.

ğŸ§­ Architecture Workflow

The solution follows a modern data lakehouse pattern implemented with Azure cloud services:

ğŸ—‚ï¸ Data Source

AdventureWorks data (CSV/Parquet files, HTTP API, or SQL Database)

ğŸ—ï¸ Data Ingestion â€” Azure Data Factory

Extracts data from multiple sources (on-premise/online)

Loads it into Azure Data Lake Storage Gen2 (Raw Zone)

ğŸ’¾ Raw Data Store â€” Data Lake Gen2

Stores raw, unprocessed data for traceability

Acts as the Bronze Layer

âš™ï¸ Data Transformation â€” Azure Databricks

Cleans, transforms, and enriches data

Writes output to Transformed Zone (Silver/Gold Layer) in Data Lake Gen2

ğŸ§® Data Serving â€” Azure Synapse Analytics

Creates analytical models and views from transformed data

Prepares data for reporting and analytics

ğŸ“Š Data Visualization â€” Power BI

Connects to Synapse for interactive dashboards and reports

ğŸ§° ****** TECH STACK ********
1.Component Service Purpose 
2.Ingestion Azure Data Factory Pipeline orchestration 
3.Storage Azure Data Lake Gen2 Raw & transformed data 
4.Processing Azure Databricks Data transformation using PySpark 
5.Serving Azure Synapse Analytics Data modeling and querying 
6.Reporting Power BI Business intelligence dashboards
